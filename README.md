# ML-Projects
Repository of all ML Projects 

All these projects are also in Google Colab link below:
https://drive.google.com/drive/folders/18MhVFDuj5qV7GAFVKU06jl9YF23ghUl_?usp=sharing

This Repository is a part of my online Data Science Portfolio.

It is mainly Machine Learning Projects using Python -- Supervised, Semi-Supervised and Unsupervised Learning -- TensorFlow and Keras still in early Phase! Here are the current projects

Current and most active Projects!

CLASSIFICATION PROJECT (Recent and still on going)

LINEAR REGRESSION PROJECTS (Recent and still on going) - Most up to date!

The Below Projects are in draft version and still need polishing! After completion it will move up to the CURRENT Projects!

2. Logic Regression Project

3. K Nearest Neighbors

4. Decision Trees & Ransom Forest

5. Support Vectors Machines

6. K Means Clustering 

7. Recommender Systems 

Still in early Phase!
8. TensorFlow and Keras

-- Project Status: [Active]

Project Intro/Objective

The purpose of this project is for showcasing some of the Machine Learning skills to potential companies or possible project collaborators. 

The main goal of this project is also to document my learning process for getting into Data Science and to use the code as reference for future projects. 
The data is all publically availible dataset and the evaluations of the data is done based on a combination of online courses, books, kaggle references, and self data exploration. The main goal is predictive analytics using Machine Learning tools and techniques. 

Step 1: Data collection/Open Data

For my projects I am mainly skipping the collections and relying on pubically availible data. This does not mean that the data is clean, some data manipulation for the projects are still needed especially with respect to some parameters and the data requires quite some feature engineering. Machines like numerical data so one has to work around this issue doing some feature engineering. 


Step 2: Data Exploration and Profiling
 
Once the data is  imported, it’s time to assess the condition of it, including looking for trends, outliers, exceptions, incorrect, inconsistent, missing, or skewed information. This is important because your source data will inform all of your model’s findings, so it is critical to be sure it does not contain unseen biases. 

Step 3: Formatting data to make it consistent
 
The next step in great data preparation is to ensure your data is formatted in a way that best fits your machine learning model. 

Step 4: Improving data quality
 
Next step is to develop a strategy for dealing with erroneous data, missing values, extreme values, and outliers in your data. 

Step 5: Feature engineering
 
This step involves the art and science of transforming raw data into features that better represent a pattern to the learning algorithms. For example, data can be decomposed into multiple parts to capture more specific relationships, such as analyzing sales performance by the day of the week, not only the month or year. 

Step 6: Splitting data into training and evaluation sets
 
The final step is to split your data into two sets; one for training your algorithm, and another for evaluation purposes. Be sure to select non-overlapping subsets of your data for the training and evaluation sets in order to ensure proper testing. Invest in tools that provide versioning and cataloging of your original source as well as your prepared data for input to machine learning algorithms, and the lineage between them. This way, you can trace the outcome of your predictions back to the input data to refine and optimize your models over time.


 
Data preparation has long been recognized for helping business leaders and analysts to ready and prepare the data needed for analytics, operations, and regulatory requirements. Self-service data preparation that runs on Amazon Web Services (AWS) and Azure takes it to the next level by leveraging many valuable attributes of a cloud-based environment.

As a result, business users who are closest to the data and most knowledgeable about its business context, can prepare data sets quickly and accurately, with the help of built-in intelligence and smart algorithms. They can work within an intuitive, visual application to access, explore, shape, collaborate and publish data with clicks, not code, with complete governance and security. 

Solutions like DP solve many data challenges and enable ML and data science workflows that enhance applications with machine intelligence. More importantly, it enables them to transform data into information on-demand to empower every person, process, and system in the organization to be more intelligent.

Technologies:
Python 3
Pandas, Numpy, Matplotlib, Seaborn, sklearn, Tensorflow 2.0 and Keras
Jupyter Notebook


Needs of these project:

data exploration/descriptive statistics
data processing/cleaning
statistical modeling
writeup/reporting

(Contacts) : James A. Odendal PhD https://github.com/jamesodendal/ML-Projects

Linkedin:
https://www.linkedin.com/in/jamesodendal/

